# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hpLF-9LO7Dvb-j_75hAFtxUgfskyqiMS

# Libraries
"""

import numpy as np
import pandas as pd
from sklearn import metrics, preprocessing
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
!pip install kmodes
from kmodes.kmodes import KModes
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve, auc

"""# Read dataset"""

df = pd.read_csv("/content/cardio_train.csv")

# Checking if the dataset is imported successfully
df.sample(10)

df = df.drop(columns=['id'])

"""# **Data Cleaning**
**1) Identifying**
"""

# Determine number of columns and rows needed
num_cols = len(df.columns)
num_rows = (num_cols - 1) // 12 + 1

# Create subplots
for row in range(num_rows):
    plt.figure(figsize=(16, 12))
    for i, column in enumerate(df.columns[row*12:min((row+1)*12, num_cols)]):
        plt.subplot(4, 3, i + 1)
        sb.boxplot(data=df[column])
        plt.title(column)
    plt.tight_layout()
    plt.show()

"""* Here, the presence of outliers in the dataset is evident, which are observed in "height", "weight", "ap_hi" and "ap_lo".
* These outliers may have been the result of errors in data entry.
* The removal of these outliers has the potential to improve the performance of our predictive model.

**2) Removal of Outliers**

* Obtain statistical information from the box plot
* First and third quartile gives the box limits
* Inter Quartile Range can be calculated based on the difference between first and third quartile
"""

df.shape

def statistic_Info(flier):
    Q1 = np.percentile(df[flier],25)
    Q3 = np.percentile(df[flier],75)
    IQR = Q3 - Q1
    lower_Whisker = np.max([df[flier].min(), Q1 - 1.5*IQR])
    upper_Whisker = np.min([df[flier].max(), Q3 + 1.5*IQR])
    return Q1, Q3, IQR, lower_Whisker, upper_Whisker

"""i. **Height**"""

df['height'].describe()

q1,q3,k,low,high = statistic_Info("height")
print(q1, q3, k, low, high)

# Finding Outliers
numFliers = df[(df['height'] > high) | (df['height'] < low)].shape[0]
print("No. of fliers: ", numFliers)

# Total Fliers
totalFliers = numFliers
print("Total fliers: ", totalFliers)

# Trim outliers
df = df[(df['height'] <= high) & (df['height'] >= low)]

# Checking if outliers are successfully removed
numFliers = df[(df['height'] > high) | (df['height'] < low)].shape[0]
print("No. of fliers after trimming: ", numFliers)

"""Number of rows before trimming = 70000

Number of outliers = 519

Available rows = 69481

Total number of outliers  found and removed = 519

ii. **Weight**
"""

df['weight'].describe()

q1,q3,k,low,high = statistic_Info("weight")
print(q1, q3, k, low, high)

# Finding Outliers
numFliers = df[(df['weight'] > high) | (df['weight'] < low)].shape[0]
print("Number of outliers in 'weight' column:", numFliers)

# Total Fliers
totalFliers = totalFliers + numFliers
print("Total number of outliers so far:", totalFliers)

# Checking available rows
print("Number of rows before trimming outliers:", df.shape[0])

# Trimming outliers
df = df[(df['weight'] >= low) & (df['weight'] <= high)]
print("Number of rows after trimming outliers:", df.shape[0])

# Checking if outliers are successfully removed
numFliers = df[(df['weight'] > high) | (df['weight'] < low)].shape[0]
print("Number of outliers in 'weight' column after trimming:", numFliers)

"""**iii. ap_hi**"""

df['ap_hi'].describe()

q1,q3,k,low,high = statistic_Info("ap_hi")
print(q1, q3, k, low, high)

# Finding Outliers
numFliers = df[(df['ap_hi'] > high) | (df['ap_hi'] < low)].shape[0]
print("Number of outliers in 'ap_hi' column:", numFliers)

# Total Fliers
totalFliers = totalFliers + numFliers
print("Total number of outliers so far:", totalFliers)

# Checking available rows
print("Number of rows before trimming outliers in 'ap_hi' column:", df['ap_hi'].shape[0])

# Trimming outliers
df = df[(df['ap_hi'] <= high) & (df['ap_hi'] >= low)]
print("Number of rows after trimming outliers in 'ap_hi' column:", df.shape[0])

# Checking if outliers are successfully removed
numFliers = df[(df['ap_hi'] > high) | (df['ap_hi'] < low)].shape[0]
print("Number of outliers in 'ap_hi' column after trimming:", numFliers)

"""**iv. ap_lo**"""

df['ap_lo'].describe()

q1,q3,k,low, high = statistic_Info("ap_lo")
print(q1,q3,k,low, high)

#Finding outliers
numFliers = df[(df['ap_lo']<low) | (df['ap_lo']>high)].shape[0]
print("Number of outliers in 'ap_lo' column:", numFliers)

#total fliers
totalFliers = totalFliers + numFliers
print("Total number of outliers so far:", totalFliers)

# Checking available rows
print("Number of rows before trimming outliers in 'ap_lo' column:", df['ap_lo'].shape[0])

#trimming outliers
df = df[(df['ap_lo']>=low) & (df['ap_lo']<=high)]
print("Number of rows after trimming outliers in 'ap_lo' column:", df.shape[0])

#checking if outliers are successfully removed
numFliers = df[(df['ap_lo']>high) | (df['ap_lo']<low)].shape[0]
print("Number of outliers in 'ap_lo' column after trimming:", numFliers)

"""**Observation**

* Total Number of Rows before trimming outliers = 70,000

* Attributes included = {"height", "weight", "ap_hi", "ap_lo"}

* Total number of outliers detected and removed = 7,495

* Number of available rows (free of outliers) = 62,505

**3.Checking Null Values**
"""

df.info()

df.isna().sum()

"""**4. Checking Missing Data**"""

for col in df.columns:
    print(f"Attr : {col}\n",end=" ")
    print(df[col].count())
    print(df[col].unique())
    print("----------------------------------------------")

"""**5. Save the dataset in a new CSV File**"""

df.to_csv('cleaned_dataset.csv', index=False, encoding='utf-8')

"""# **Feature Selection & Reduction**"""

# Read the cleaned dataset ".csv" file imported to the folder
df = pd.read_csv("cleaned_dataset.csv")
dummy = pd.read_csv("cleaned_dataset.csv")

# Checking if the dataset is imported successfully
df.head(10)

"""**AGE**"""

#Converting number of days into years
df["age"] = round(df["age"]/365.0)

#Calculating interval
min_Age = int(df["age"].min())
max_Age = int(df["age"].max())
print(min_Age, max_Age)

#defining interval, bins and labels
interval = 5
age_bins = [30,35,40,45,50,55,60,70]
age_labels = list(i for i in range(len(age_bins)-1))
print(f"{age_bins}\n{age_labels}")

#binning using age_bins and age_labels
# pd.cut(x, bins=[], labels=[], right=True/False, include_lowest=True/False, ...)

df["age"] = pd.cut(df["age"],bins=age_bins, labels=age_labels, include_lowest=True, right=False)
df.head(10)

"""**BMI**

Body Mass Index is the weight of the bodyh proportion to the square of height, calculated in either metric system or imperical system.

Underweight: BMI less than 18.5

Normal weight: BMI 18.5 to 24.9

Overweight: BMI 25 to 29.9

Obesity Class I (Moderate): BMI 30 to 34.9

Obesity Class II (Severe): BMI 35 to 39.9

Obesity Class III (Very Severe or Morbid Obesity): BMI 40 or greater
"""

#Converting units of centimetres into metres for attribute "height"
df["height"] = df["height"]/100.0

#calculate BMI

df["BMI"] = round(df["weight"]/(df["height"]**2),1)

df = df.drop(["height", "weight"], axis=1)

min_bmi = min(df["BMI"])
max_bmi = max(df["BMI"])
print(f"{min_bmi}\t{max_bmi}")

# Defining bins based on the known BMI ranges

bmi_bins = [0,18.5,25,30,35,40,100]
bmi_labels = [i for i in range(len(bmi_bins)-1)]
print(f"{bmi_bins}\n{bmi_labels}")

# Binning
df["BMI"] = pd.cut(df["BMI"],bins=bmi_bins,labels=bmi_labels,right=False,include_lowest=True)
df.head(10)

"""**MAP**

Mean Arterial Pressure defines the average blood pressure of a person in one cardiac cycle.

HypoTension Crisis : Below 70

HypoTension : 70 to 80

Normal : 80 to 90

Elevated/Pre HyperTension: 90 to 100

Hyper Tension Stage I: 100 to 110

Hyper Tension StageII: 110 to 120

Hyper Tension Crisis : Above 120
"""

df["MAP"] = round(((2*df["ap_lo"])+df["ap_hi"])/3)
df = df.drop(["ap_lo","ap_hi"],axis=1)
df.head(10)

min_map = min(df["MAP"])
max_map = max(df["MAP"])
print(f"{min_map}\t{max_map}")

# Defining bin values based on the above criteria
map_bins = [0,70,80,90,100,110,140]
map_labels = list(i for i in range(len(map_bins)-1))
print(f"{map_bins}\n{map_labels}")

#binning
df["MAP"] = pd.cut(df['MAP'],bins=map_bins,labels=map_labels,include_lowest=True,right=False)
df.head(10)

"""**Checking MISSING Data**"""

df.isna().sum()

"""**Data Transformation**"""

df.describe()

labelEncoder = preprocessing.LabelEncoder()
df = df.apply(labelEncoder.fit_transform)
for col in df.columns:
    print(f"Attr : {col}\n",end=" ")
    print(df[col].count())
    print(df[col].unique())
    print("----------------------------------------------")

# Identify rows containing NaN values
nan_rows = df[df.isna().any(axis=1)]

# Print entire rows containing NaN values
for index, row in nan_rows.iterrows():
    print(row)
    print("-----------------------------------------------------------------")

df.describe()

"""**Save the dataset in new csv file**"""

df.to_csv('transformed_Dataset.csv', index=False, encoding='utf-8')

"""# **Clustering and Modeling**"""

# Read the transformed dataset ".csv" file imported to the folder
df = pd.read_csv("/content/transformed_Dataset.csv")

"""**Elbow Curve Method**"""

cost = []
num_clusters = range(1,6) # 1 to 5
for i in list(num_clusters):
    kmode = KModes(n_clusters=i, init = "Huang", n_init = 5, verbose=0,random_state=1)
    kmode.fit_predict(df)
    cost.append(kmode.cost_)

plt.plot(num_clusters, cost, 'bo-')
plt.xlabel('num_clusters')
plt.ylabel('Cost')
plt.title('Elbow Method For Optimal Number of Clusters')
plt.show()

"""Optimal Number of clusters: 2

**K Modes Clustering**
"""

km = KModes(n_clusters=2, init = "Huang", n_init = 5,random_state=1)
clusters = km.fit_predict(df)

"""**Correlation Matrix**"""

df.insert(0,"clusters",clusters,True)
df.head(10)

plt.figure(figsize=(10, 8))

# Draw correlation matrix
sb.heatmap(df.corr(), annot=True, cmap='Spectral', fmt=".2f", linewidths=.5)

# Show the figure
plt.title('Correlation Matrix')
plt.show()

sb.countplot(x='clusters', hue='cardio', data=df)
plt.title('Distribution of Cardiovascular Disease within Clusters')
plt.show()

"""**TRAINING**"""

x = df.drop(['cardio','gender','alco'], axis=1)
y = df['cardio']

x.head()

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=1)
x_train_xg,x_test_xg,y_train_xg,y_test_xg=train_test_split(x,y,test_size=0.20,random_state=1)
x_train_mlp,x_test_mlp,y_train_mlp,y_test_mlp=train_test_split(x,y,test_size=0.20,random_state=1)
x_train_rf,x_test_rf,y_train_rf,y_test_rf=train_test_split(x,y,test_size=0.20,random_state=1)
x_train_dt,x_test_dt,y_train_dt,y_test_dt=train_test_split(x,y,test_size=0.20,random_state=1)

"""#**MODELING**

**RF**
"""

# Define features (x) and target (y)
x = df.drop(['cardio', 'gender', 'alco'], axis=1)
y = df['cardio']

# Random Forest model
rf_model = RandomForestClassifier(random_state=1)

# Fit the model
rf_model.fit(x_train, y_train)

# Make predictions
rf_pred = rf_model.predict(x_test)

# Accuracy
rf_accuracy = metrics.accuracy_score(y_test, rf_pred) * 100
print(f"Accuracy without CV: {rf_accuracy:.2f}")

# Define a simplified parameter grid for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20],
    'min_samples_split': [5, 10],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', None],
}

# Create grid search for Random Forest
rf_gridsearch = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)

# Fit grid search for Random Forest
rf_gridsearch.fit(x_train, y_train)

best_params = rf_gridsearch.best_params_
best_estimator = rf_gridsearch.best_estimator_

print(f"Best Parameters : {best_params}")
print(f"Best Estimator  : {best_estimator}")

rf_pred_CV = best_estimator.predict(x_test_rf)

rf_accuracy_cv = metrics.accuracy_score(y_test_rf, rf_pred_CV)*100
print(f"Best Accuracy: {rf_accuracy_cv:.2f}")

print(f"Random Forest accuracy without CV : {rf_accuracy:.2f}")
print(f"Random Forest accuracy with CV    : {rf_accuracy_cv:.2f}")

classification_report_str = classification_report(y_test, rf_pred_CV, digits=4)

print("Classification Report for RF with CV:\n", classification_report_str)

"""# **MLP**"""

#mlp

# build MLP model
mlpModel = MLPClassifier(random_state=1)

# Fit the model
mlpModel.fit(x_train_mlp, y_train_mlp)

# Make predictions
mlp_pred = mlpModel.predict(x_test_mlp)

# accuracy
mlp_accuracy = metrics.accuracy_score(y_test_mlp, mlp_pred)*100
print(f"Accuracy without CV: {mlp_accuracy:.2f}")

# Best parameters for MLP
mlp_best_params = {
    'activation': ['tanh'],
    'alpha': [0.01],
    'hidden_layer_sizes': [(50, 50)],
    'max_iter': [300],
    'solver': ['adam'],
}

# Create grid search
mlp_gridsearch = GridSearchCV(estimator=mlpModel, param_grid=mlp_best_params, cv=5, scoring='accuracy', n_jobs=-1)

# Fit grid search
mlp_gridsearch.fit(x_train_mlp, y_train_mlp)

# mlp_best_params = mlp_gridsearch.best_params_
mlp_best_estimator = mlp_gridsearch.best_estimator_

print(f"Best Parameters : {mlp_best_params}")
print(f"Best Estimator  : {mlp_best_estimator}")

mlp_pred_CV = mlp_best_estimator.predict(x_test_mlp)

mlp_accuracy_cv = metrics.accuracy_score(y_test_mlp, mlp_pred_CV)*100
print(f"Best Accuracy: {mlp_accuracy_cv:.2f}")

print(f"MLP accuracy without CV : {mlp_accuracy:.2f}")
print(f"MLP accuracy with CV    : {mlp_accuracy_cv:.2f}")

classification_report_str = classification_report(y_test, mlp_pred_CV, digits=4)

print("Classification Report for MLP with CV:\n", classification_report_str)

"""# **DT**"""

#dt

# build MLP model
dtModel = DecisionTreeClassifier(random_state=1)

# Fit the model
dtModel.fit(x_train_dt, y_train_dt)

# Make predictions
dt_pred = dtModel.predict(x_test_dt)

# accuracy
dt_accuracy = metrics.accuracy_score(y_test_dt, dt_pred)*100
print(f"Accuracy without CV: {dt_accuracy:.2f}")

# Best parameters for dt
dt_best_params = {
    'criterion': ['entropy'],
    'max_depth': [6],
   # 'n_estimators': [100],
    #'subsample': [0.8],
    #'colsample_bytree': [0.8],

}

# Create grid search
dt_gridsearch = GridSearchCV(estimator=dtModel, param_grid=dt_best_params, cv=5, scoring='accuracy', n_jobs=-1)

# Fit grid search
dt_gridsearch.fit(x_train_dt, y_train_dt)

dt_best_estimator = dt_gridsearch.best_estimator_

print(f"Best Parameters : {dt_best_params}")
print(f"Best Estimator  : {dt_best_estimator}")

dt_pred_CV = dt_best_estimator.predict(x_test_dt)

dt_accuracy_cv = metrics.accuracy_score(y_test_dt, dt_pred_CV)*100
print(f"Best Accuracy with cv: {dt_accuracy_cv:.2f}")

"""# **XGBoost**"""

#xgboost
xgbmodel = XGBClassifier(random_state=1)


# Fit the model
xgbmodel.fit(x_train_xg, y_train_xg)

# Make predictions
xgbpred = xgbmodel.predict(x_test_xg)

# accuracy
xgbaccuracy = metrics.accuracy_score(y_test_xg, xgbpred)*100
print(f"Accuracy without CV: {xgbaccuracy:.2f}")

print(x_train_xg.shape, y_train_xg.shape)

param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'n_estimators': [50, 100, 200],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],

}

best_xgbparam = {
    'learning_rate': [0.1],
    'max_depth': [3],
    'n_estimators': [100],
    'subsample': [0.8],
    'colsample_bytree': [0.8],

}

grid_search = GridSearchCV(estimator=xgbmodel, param_grid=best_xgbparam, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(x_train_xg, y_train_xg)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

print(f'Best parameters: {best_params}')
print(f"Best Estimator  : {best_model}")

xgb_pred_CV = best_model.predict(x_test_xg)

xgb_accuracy_cv = metrics.accuracy_score(y_test_xg, xgb_pred_CV)*100
print(f"Best Accuracy with CV: {xgb_accuracy_cv:.2f}")

"""# **Performance Metric Evaluation**

**Accuracy without CV**
"""

print(f"RF accuracy without CV : {rf_accuracy:.2f}")
print(f"mlp accuracy without CV : {mlp_accuracy:.2f}")
print(f"Dt accuracy without CV : {dt_accuracy:.2f}")
print(f"XGB accuracy without CV: {xgbaccuracy:.2f}")

"""**AUC Score without CV**"""

# Predict probabilities
rf_probs = rf_model.predict_proba(x_test)[:, 1]
dt_probs = dtModel.predict_proba(x_test)[:, 1]
mlp_probs = mlpModel.predict_proba(x_test)[:, 1]
xgb_probs = xgbmodel.predict_proba(x_test)[:, 1]

# Calculate AUC scores
rf_auc = roc_auc_score(y_test_rf, rf_probs)
dt_auc = roc_auc_score(y_test_dt, dt_probs)
mlp_auc = roc_auc_score(y_test_mlp, mlp_probs)
xgb_auc = roc_auc_score(y_test_xg, xgb_probs)

# Print AUC scores
print("Random Forest AUC:", round(rf_auc,2))
print("Decision Tree AUC:", round(dt_auc,2))
print("Multi-Layer Perceptron AUC:", round(mlp_auc,2))
print("XGBoost AUC:", round(xgb_auc,2))

"""**F1 Score without CV**"""

rf_f1 = f1_score(y_test_rf, rf_pred)
dt_f1 = f1_score(y_test_dt, dt_pred)
mlp_f1 = f1_score(y_test_mlp, mlp_pred)
xgb_f1 = f1_score(y_test_xg, xgbpred)

# Print F1 score, recall, and precision
print("Random Forest F1 score without CV:", round(rf_f1*100,2))
print("Decision Tree F1 score without CV:", round(dt_f1*100,2))
print("Multi-Layer Perceptron F1 score without CV:", round(mlp_f1*100,2))
print("XGBoost F1 score without CV:", round(xgb_f1*100,2))

"""**Recall without CV**"""

rf_recall = recall_score(y_test_rf, rf_pred)
dt_recall = recall_score(y_test_dt, dt_pred)
mlp_recall = recall_score(y_test_mlp, mlp_pred)
xgb_recall = recall_score(y_test_xg, xgbpred)

print("Random Forest Recall without CV:", round(rf_recall*100,2))
print("Decision Tree Recall without CV:", round(dt_recall*100,2))
print("Multi-Layer Perceptron Recall without CV:", round(mlp_recall*100,2))
print("XGBoost Recall without CV:", round(xgb_recall*100,2))

"""**Precision without CV**"""

rf_precision = precision_score(y_test_rf, rf_pred)
dt_precision = precision_score(y_test_dt, dt_pred)
mlp_precision = precision_score(y_test_mlp, mlp_pred)
xgb_precision = precision_score(y_test_xg, xgbpred)

print("Random Forest Precision without CV:", round(rf_precision*100,2))
print("Decision Tree Precision without CV:", round(dt_precision*100,2))
print("Multi-Layer Perceptron Precision without CV:", round(mlp_precision*100,2))
print("XGBoost Precision: without CV", round(xgb_precision*100,2))

"""**Accuracy with CV**"""

print(f"RF accuracy with CV    : {rf_accuracy_cv:.2f}")
print(f"mlp accuracy with CV    : {mlp_accuracy_cv:.2f}")
print(f"Dt accuracy with CV    : {dt_accuracy_cv:.2f}")
print(f"XGB accuracy with CV    : {xgb_accuracy_cv:.2f}")

"""**F1 Score with CV**"""

rf_f1_cv = cross_val_score(rf_model, x, y, cv=5, scoring='f1').mean()
dt_f1_cv = cross_val_score(dtModel, x, y, cv=5, scoring='f1').mean()
mlp_f1_cv = cross_val_score(mlpModel, x, y, cv=5, scoring='f1').mean()
xgb_f1_cv = cross_val_score(xgbmodel, x, y, cv=5, scoring='f1').mean()

# Print cross-validated F1 score
print("Random Forest Cross-Validated F1 score:", round(rf_f1_cv*100,2))
print("Decision Tree Cross-Validated F1 score:", round(dt_f1_cv*100,2))
print("Multi-Layer Perceptron Cross-Validated F1 score:", round(mlp_f1_cv*100,2))
print("XGBoost Cross-Validated F1 score:", round(xgb_f1_cv*100,2))

"""**Recall with CV**"""

f_recall_cv = cross_val_score(rf_model, x, y, cv=5, scoring='recall').mean()
dt_recall_cv = cross_val_score(dtModel, x, y, cv=5, scoring='recall').mean()
mlp_recall_cv = cross_val_score(mlpModel, x, y, cv=5, scoring='recall').mean()
xgb_recall_cv = cross_val_score(xgbmodel, x, y, cv=5, scoring='recall').mean()

print("Random Forest Cross-Validated Recall:", round(f_recall_cv*100,2))
print("Decision Tree Cross-Validated Recall:", round(dt_recall_cv*100,2))
print("Multi-Layer Perceptron Cross-Validated Recall:", round(mlp_recall_cv*100,2))
print("XGBoost Cross-Validated Recall:", round(xgb_recall_cv*100,2))

"""**Precision with CV**"""

rf_precision_cv = cross_val_score(rf_model, x, y, cv=5, scoring='precision').mean()
dt_precision_cv = cross_val_score(dtModel, x, y, cv=5, scoring='precision').mean()
mlp_precision_cv = cross_val_score(mlpModel, x, y, cv=5, scoring='precision').mean()
xgb_precision_cv = cross_val_score(xgbmodel, x, y, cv=5, scoring='precision').mean()

print("Random Forest Cross-Validated Precision:", round(rf_precision_cv*100,2))
print("Decision Tree Cross-Validated Precision:", round(dt_precision_cv*100,2))
print("Multi-Layer Perceptron Cross-Validated Precision:", round(mlp_precision_cv*100,2))
print("XGBoost Cross-Validated Precision:", round(xgb_precision_cv*100,2))